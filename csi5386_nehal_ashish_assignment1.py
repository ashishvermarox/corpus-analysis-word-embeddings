# -*- coding: utf-8 -*-
"""CSI5386_Nehal_Ashish_Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KtxBYn-42aWCLHTBaquOvx_duOqnEvQr

# CSI5386 - Natural Language Processing | Assignment 1

### **Nehal Aggarwal | 300092092** | nagga097@uottawa.ca

### **Ashish Verma | 300059114** | averm019@uottawa.ca 

## **University Of Ottawa**

### Mounting Google Drive
"""

# Load the Drive helper and mount
from google.colab import drive

drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd /content/drive/My Drive/Term5/NLP/Dataset
!ls

"""### Helper Methods"""

def outputToFileWithColumn(data, outputPath, filename, columnToSave):
  header = [columnToSave]
  data.to_csv(outputPath+filename, index=False, mode='a', columns = header)
  print('File:'+filename+' stored at'+' Path:'+outputPath)

def outputToFile(data, outputPath, filename):
  data.to_csv(outputPath+filename, index=False, mode='a')
  print('File:'+filename+' stored at'+' Path:'+outputPath)

def readFile(filePath):
  file = open(filePath)
  raw_text = file.read().splitlines()
  file.close()
  return raw_text

def readFileBuffer(filePath):
  file = open(filePath)
  raw_text = file.read()
  file.close()
  return raw_text

def writeToFile(data, outputPath, filename):
  file = open(outputPath+filename, "w")
  file.write(data)
  return True

def writeToFileList(data, outputPath, filename):
  with open(outputPath+filename, "w") as f:
    for tokens in data:
        f.write("%s\n" % tokens)
  return True

"""### Cleaning Tweets"""

import re
def removeURL(text):
  text = re.sub(r'\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*', '', text)
  return text

def removeDigits(text):
  text = re.sub(r'\d+', '', text)
  return text

def removeHashtags(text):
  text = " ".join(filter(lambda x:x[0]!='#', text.split()))
  return text

def removeHTMLTags(text):
  clean = re.compile('<.*?>')
  text = re.sub(clean, '', text)
  return text

def toLowerCase(text):
  text = text.lower()
  return text

def removeReTweetWord(text):
  text = " ".join([word for word in text.split() if not word == 'rt'])
  return text

!pip install contractions

import contractions

def replaceContractions(text):
  text = contractions.fix(text)
  return text

from nltk.tokenize import RegexpTokenizer

def removePunctuationMarks(text):
  tokenizer = RegexpTokenizer(r'\w+')
  result = tokenizer.tokenize(text)
  nopuncttext = " ".join(result)
  return nopuncttext

def removeNonEnglishChars(text):
  #words = set(nltk.corpus.words.words())
  #text = " ".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())
  #return text
  text = re.sub(r'[^\x00-\x7f]',r'', text)
  return text

def removeUnderscore(text):
  text = text.replace("_", "");
  return text

"""### Import NLTK Libraries"""

import nltk
nltk.download("popular")

import pandas as pd

"""### Load Corpus"""

dataPath = '/content/drive/My Drive/Term5/NLP/Dataset'

outputPath = '/content/drive/My Drive/Term5/NLP/Output'

corpus = dataPath + '/microblog2011.txt'

stopWordsPath = dataPath + '/StopWords.txt'

data = readFile(corpus)

"""#### Create tokens from clean data"""

from nltk.tokenize import TweetTokenizer

tweetTokenize = TweetTokenizer(strip_handles=True)

tweet_tokenized_text = []
for eachTweet in data:
  tokenizedTweet = tweetTokenize.tokenize(eachTweet)
  tweet_tokenized_text.append(tokenizedTweet)

filename = '/tweet_tokenized_text.txt'
writeToFileList(tweet_tokenized_text, outputPath, filename)

cleaned_tweets = []
for singleTweet in tweet_tokenized_text:
  tmpTweet = " ".join(singleTweet)
  texttweet = removeURL(tmpTweet)
  #texttweet = removeDigits(texttweet) ##check if digits should be removed or not
  #texttweet = removeHashtags(texttweet)
  texttweet = removeHTMLTags(texttweet)
  texttweet = toLowerCase(texttweet)
  texttweet = replaceContractions(texttweet)
  #texttweet = removeNonEnglishChars(texttweet)
  #texttweet = removePunctuationMarks(texttweet)
  #texttweet = removeUnderscore(texttweet)
  texttweet = removeReTweetWord(texttweet)
  cleaned_tweets.append(texttweet)

filename = '/clean_tweets.txt'
writeToFileList(cleaned_tweets, outputPath, filename)

"""#### Part 1: Corpus processing: tokenization, and word counting

##### Part a
"""

raw_data = readFileBuffer(outputPath+'/clean_tweets.txt')

clean_tokens = nltk.word_tokenize(raw_data)

filename = '/microblog2011_tokenized.txt'
writeToFileList(clean_tokens, outputPath, filename)

"""#### Part b"""

numOfTokens = len(clean_tokens)
print("Number of Tokens in corpus are: "+str(numOfTokens))

uniqueTokens = len(set(clean_tokens))
print("Number of Unique Tokens in corpus are: "+str(uniqueTokens))

typeTokenRatio = uniqueTokens/numOfTokens
print("Type token ratio of corpus is: "+str(typeTokenRatio))

"""#### Part c"""

eachFreq = ''
filename = '/Tokens.txt'
freqDistb = nltk.FreqDist(clean_tokens)
print(freqDistb)
#freqDistb.tabulate()
#type(freqDistb)
for word, frequency in freqDistb.most_common():
    eachFreq = eachFreq + '\n'+ (u'{}:{}'.format(word, frequency))
writeToFile(eachFreq, outputPath, filename)

"""#### Part d"""

#print(freqDistb.items())
max_occurance = 1
only_once = [k for k,c in freqDistb.items() if c == max_occurance]
print(len(only_once))

"""#### Part e"""

fully_cleaned_tweets = []
for singleTweet in tweet_tokenized_text:
  tmpTweet = " ".join(singleTweet)
  texttweet = removeURL(tmpTweet)
  texttweet = removeDigits(texttweet)
  texttweet = removeHashtags(texttweet)
  texttweet = removeHTMLTags(texttweet)
  texttweet = toLowerCase(texttweet)
  texttweet = replaceContractions(texttweet)
  texttweet = removeNonEnglishChars(texttweet)
  texttweet = removePunctuationMarks(texttweet)
  texttweet = removeUnderscore(texttweet)
  texttweet = removeReTweetWord(texttweet)
  fully_cleaned_tweets.append(texttweet)

filename = '/fully_clean_tweets.txt'
writeToFileList(fully_cleaned_tweets, outputPath, filename)

only_words_raw_data = readFileBuffer(outputPath+'/fully_clean_tweets.txt')

word_clean_tokens = nltk.word_tokenize(only_words_raw_data)

numOfTokensCleaned = len(word_clean_tokens)
print("Number of Tokens in corpus with only words are: "+str(numOfTokensCleaned))

eachFreq = ''
filename = '/Tokens_cleaned.txt'
freqDistbCleaned = nltk.FreqDist(word_clean_tokens)
print(freqDistbCleaned)
#freqDistb.tabulate()
#type(freqDistb)
for word, frequency in freqDistbCleaned.most_common(100):
    eachFreq = eachFreq + '\n'+ (u'{}:{}'.format(word, frequency))
    print(u'{}:{}'.format(word, frequency))
writeToFile(eachFreq, outputPath, filename)

uniqueTokensCleaned = len(set(word_clean_tokens))
print("Number of Unique Tokens in corpus with only words are: "+str(uniqueTokensCleaned))

typeTokenRatioCleaned = uniqueTokensCleaned/numOfTokensCleaned
print("Type token ratio of corpus with only words is: "+str(typeTokenRatioCleaned))

"""#### Part f"""

from nltk.corpus import stopwords 
stop_words = set(stopwords.words('english'))
filtered_tokens = [w for w in word_clean_tokens if not w in stop_words]

eachFreq = ''
filename = '/Tokens_cleaned_stopwords.txt'
freqDistbCleanedStopWords = nltk.FreqDist(filtered_tokens)
print(freqDistbCleanedStopWords)
#freqDistb.tabulate()
#type(freqDistb)
for word, frequency in freqDistbCleanedStopWords.most_common(100):
    eachFreq = eachFreq + '\n'+ (u'{}:{}'.format(word, frequency))
    print(u'{}:{}'.format(word, frequency))
writeToFile(eachFreq, outputPath, filename)

numOfTokensCleanedStopWords = len(filtered_tokens)
print("Number of Tokens in corpus with only words and excluding stop words are: "+str(numOfTokensCleanedStopWords))

uniqueTokensCleanedStopWords = len(set(filtered_tokens))
print("Number of Unique Tokens in corpus with only words and excluding stop words are: "+str(uniqueTokensCleanedStopWords))

typeTokenRatioCleanedStopWords = uniqueTokensCleanedStopWords/numOfTokensCleanedStopWords
print("Type token ratio of corpus with only words and excluding stop words is: "+str(typeTokenRatioCleanedStopWords))

"""#### Part g"""

bigram = nltk.bigrams(filtered_tokens)

eachFreq = ''
filename = '/Bigrams_cleaned_stopwords.txt'
freqDistbBigrams = nltk.FreqDist(bigram)
print(freqDistbBigrams)
#freqDistb.tabulate()
#type(freqDistb)
for word, frequency in freqDistbBigrams.most_common(100):
    eachFreq = eachFreq + '\n'+ (u'{}:{}'.format(word, frequency))
    print(u'{}:{}'.format(word, frequency))
writeToFile(eachFreq, outputPath, filename)

"""#### Part 2: Evaluation word embeddings"""

!pip install -r requirements.txt

"""## Helper Method"""

def calSimilarityResult(tasks, w_data):
  # Calculate results using helper function
  for name, data in iteritems(tasks):
    print ("Spearman correlation of scores on {} {}".format(name, evaluate_similarity(w_data, data.X, data.y)))

"""### Similarity Tasks"""

import logging
from six import iteritems
from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999, fetch_MTurk, fetch_RG65, fetch_RW, fetch_TR9856
from web.embeddings import fetch_GloVe, fetch_static_dataset, fetch_PDC, fetch_LexVec, fetch_HDC, fetch_conceptnet_numberbatch, fetch_FastText
from web.evaluate import evaluate_similarity

tasks = {
    "MTurk": fetch_MTurk(),
    "MEN": fetch_MEN(),
    "WS353": fetch_WS353(),
    "RubensteinAndGoodenough": fetch_RG65(),
    "Rare Words": fetch_RW(),
    "SIMLEX999": fetch_SimLex999(),
    "TR9856": fetch_TR9856()
}

"""##### Fetch all word embedding models"""

wordEmbeddingPath = '/content/drive/My Drive/Term5/NLP/Dataset/WebEmbeddings'

#WE:1 GloVe from GitHub code on wiki corpus
#Corpus:Wiki
#Vocabulary Size:#
#Dimension: 300
w_gv_1 = fetch_GloVe(corpus="wiki-6B", dim=300)

#WE:2 Analysis on Gensim continuous Skipgram
#Corpus:Gigaword 5th Edition
#Vocabulary Size:209512
#Dimension: 300
path = wordEmbeddingPath + '/26/model.txt'
w_gcs_1 = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

#WE:3 Analysis on Global Vectors
#Corpus:Gigaword 5th Edition
#Vocabulary Size:209865
#Dimension: 300
path = wordEmbeddingPath + '/27/model.txt'
w_gv_2 = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

#WE:4 Analysis on Global Vectors
#Corpus:English Wikipedia Dump of February 2017
#Vocabulary Size:302815
#Dimension: 300
path = wordEmbeddingPath + '/8/model.txt'
w_gv_3 = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

#WE:5 Analysis on Global Vectors
#Corpus:English Wikipedia Dump of February 2017 | Gigaword 5th Edition
#Vocabulary Size:291392
#Dimension: 300
path = wordEmbeddingPath + '/20/model.txt'
w_gv_4 = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

#WE:6 Analysis on Gensim Continuous Bag-of-Words
#Corpus:Wikipedia
#Vocabulary Size:285055
#Dimension: 400
path = wordEmbeddingPath + '/75/model.txt'
w_gcbow_1 = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

#WE:7 Analysis on Gensim Continuous Skipgram
#Corpus:English Wikipedia Dump of February 2017
#Vocabulary Size:302866
#Dimension: 300
path = wordEmbeddingPath + '/6/model.txt'
w_gcs_2 = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

#WE:8 - Analysis on PDC embeddings trained on wiki
#Corpus:-
#Vocabulary Size:-
#Dimension: 300
w_pdc = fetch_PDC()

#WE:9 - Analysis on LexVec embeddings
w_lexvev = fetch_LexVec(which="wikipedia+newscrawl-W")

#WE:10 - Analysis on ConceptNetNumberbatch embeddings
w_concept_net = fetch_conceptnet_numberbatch()

#WE:11 - Analysis on HDC embeddings
w_hdc = fetch_HDC()

#WE:12 - FastText
#1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)
#https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip
path = wordEmbeddingPath + '/Fasttext/wiki-news-300d-1M.vec'
w_fasttext = fetch_static_dataset(path, normalize=True, lower=False, clean_words=False)

"""##### Print sample data from tasks"""

# Print sample data
for name, data in iteritems(tasks):
    print("Sample data from {}: pair \"{}\" and \"{}\" is assigned score {}".format(name, data.X[0][0], data.X[0][1], data.y[0]))

"""### Evaluate Analogy"""

from web.datasets.analogy import fetch_wordrep, fetch_semeval_2012_2, fetch_msr_analogy, fetch_google_analogy
from web.embeddings import fetch_GloVe, fetch_static_dataset
from web.analogy import *

"""#### Helper Methods"""

#Mark flag false for semeval_2012_2, since category name is different in data structure
def printCategoriesForData(data, flag):
  if flag:
    for cat in (set(data.category)):
      print(cat)
  else:
    for cat in (set(data.categories_names)):
      print(cat)

# Pick a sample of data and calculate answers
# Mark flag false for 2D data
def calAnswersOnSampleData(data, flag, w):
  subset = [50, 1000, 4000, 6000, 7000]
  if flag:
    for id in subset:
      w1, w2, w3 = data.X[id][0], data.X[id][1], data.X[id][2]
      print("Question: {} is to {} as {} is to ?".format(w1, w2, w3))
      print("Answer: " + data.y[id])
      print("Predicted: " + " ".join(w.nearest_neighbors(w[w2] - w[w1] + w[w3], exclude=[w1, w2, w3])))
  else:
    for id in subset:
      w1, w2 = data.X[id][0], data.X[id][1]
      print("Question: {} is to {} as is to ?".format(w1, w2))
      print("Answer: " + data.y[id])
      #print("Predicted: " + " ".join(w.nearest_neighbors(w[w2] - w[w1] + w[w3], exclude=[w1, w2, w3])))

def calAnswersOnWordRep(data, w, max_pairs=1000):
  categories = set(data.category)
  accuracy = {}
  correct = {}
  count = {}
  for cat in categories:
    X_cat = data.X[data.category == cat]
    X_cat = X_cat[0:max_pairs]
    print("Processing {} with {} pairs, {} questions".format(cat, X_cat.shape[0], X_cat.shape[0] * (X_cat.shape[0] - 1)))
    # For each category construct question-answer pairs
    size = X_cat.shape[0] * (X_cat.shape[0] - 1)
    X = np.zeros(shape=(size, 3), dtype="object")
    y = np.zeros(shape=(size,), dtype="object")
    id = 0
    for left, right in product(X_cat, X_cat):
      if not np.array_equal(left, right):
        X[id, 0:2] = left
        X[id, 2] = right[0]
        y[id] = right[1]
        id += 1
        solver = SimpleAnalogySolver(w=w)
        y_pred = solver.predict(X)
        correct[cat] = float(np.sum(y_pred == y))
        count[cat] = size
        accuracy[cat] = float(np.sum(y_pred == y)) / size
        # Add summary results
  correct['wikipedia'] = sum(correct[c] for c in categories if c in data.wikipedia_categories)
  correct['all'] = sum(correct[c] for c in categories)
  correct['wordnet'] = sum(correct[c] for c in categories if c in data.wordnet_categories)

  count['wikipedia'] = sum(count[c] for c in categories if c in data.wikipedia_categories)
  count['all'] = sum(count[c] for c in categories)
  count['wordnet'] = sum(count[c] for c in categories if c in data.wordnet_categories)

  accuracy['wikipedia'] = correct['wikipedia'] / count['wikipedia']
  accuracy['all'] = correct['all'] / count['all']
  accuracy['wordnet'] = correct['wordnet'] / count['wordnet']

  return pd.concat([pd.Series(accuracy, name="accuracy"),
                    pd.Series(correct, name="correct"),
                    pd.Series(count, name="count")], axis=1)

def calAnswersonSemEval(w):
  data = fetch_semeval_2012_2()
  mean_vector = np.mean(w.vectors, axis=0, keepdims=True)
  categories = data.y.keys()
  results = defaultdict(list)
  for c in categories:
     # Get mean of left and right vector
     prototypes = data.X_prot[c]
     prot_left = np.mean(np.vstack(w.get(word, mean_vector) for word in prototypes[:, 0]), axis=0)
     prot_right = np.mean(np.vstack(w.get(word, mean_vector) for word in prototypes[:, 1]), axis=0)
     questions = data.X[c]
     question_left, question_right = np.vstack(w.get(word, mean_vector) for word in questions[:, 0]), np.vstack(w.get(word, mean_vector) for word in questions[:, 1])
     scores = np.dot(prot_left - prot_right, (question_left - question_right).T)
     c_name = data.categories_names[c].split("_")[0]
     # NaN happens when there are only 0s, which might happen for very rare words or
     # very insufficient word vocabulary
     cor = scipy.stats.spearmanr(scores, data.y[c]).correlation
     results[c_name].append(0 if np.isnan(cor) else cor)
  final_results = OrderedDict()
  final_results['all'] = sum(sum(v) for v in results.values()) / len(categories)
  for k in results:
    final_results[k] = sum(results[k]) / len(results[k])
  return pd.Series(final_results)

def evaluate_analogy(w, X, y, method="add", k=None, category=None, batch_size=100):
  assert category is None or len(category) == y.shape[0], "Passed incorrect category list"
  solver = SimpleAnalogySolver(w=w, method=method, batch_size=batch_size, k=k)
  y_pred = solver.predict(X)

  if category is not None:
    results = OrderedDict({"all": np.mean(y_pred == y)})
    count = OrderedDict({"all": len(y_pred)})
    correct = OrderedDict({"all": np.sum(y_pred == y)})
    for cat in set(category):
      results[cat] = np.mean(y_pred[category == cat] == y[category == cat])
      count[cat] = np.sum(category == cat)
      correct[cat] = np.sum(y_pred[category == cat] == y[category == cat])

    return pd.concat([pd.Series(results, name="accuracy"),
                          pd.Series(correct, name="correct"),
                          pd.Series(count, name="count")],
                         axis=1)
  else:
    return np.mean(y_pred == y)

def evaluateOnAll(w):
  similarity_tasks = {
      "MTurk": fetch_MTurk(),
      "MEN": fetch_MEN(),
      "WS353": fetch_WS353(),
      "RubensteinAndGoodenough": fetch_RG65(),
      "Rare Words": fetch_RW(),
      "SIMLEX999": fetch_SimLex999(),
      "TR9856": fetch_TR9856()
    }

  similarity_results = {}
  
  for name, data in iteritems(similarity_tasks):
    similarity_results[name] = evaluate_similarity(w, data.X, data.y)
    print("Spearman correlation of scores on {} {}".format(name, similarity_results[name]))
  
  # Calculate results on analogy
  print("Calculating analogy benchmarks")
  analogy_tasks = {
        "Google": fetch_google_analogy(),
        "MSR": fetch_msr_analogy()
  }
  analogy_results = {}
  for name, data in iteritems(analogy_tasks):
    analogy_results[name] = evaluate_analogy(w, data.X, data.y)
    print("Analogy prediction accuracy on {} {}".format(name, analogy_results[name]))
  
  analogy_results["SemEval2012_2"] = calAnswersonSemEval(w)['all']
  print("Analogy prediction accuracy on {} {}".format("SemEval2012", analogy_results["SemEval2012_2"]))

  analogy = pd.DataFrame([analogy_results])
  sim = pd.DataFrame([similarity_results])
  results = sim.join(analogy)

  return results

"""#### Fetching benchmark datasets"""

# Fetch analogy dataset
data_wordrep = fetch_wordrep()
data_google = fetch_google_analogy()
data_msr = fetch_msr_analogy()
data_semeval = fetch_semeval_2012_2()

"""##### Print categories from benchmark datasets"""

printCategoriesForData(data_wordrep, True)

printCategoriesForData(data_google, True)

printCategoriesForData(data_msr, True)

printCategoriesForData(data_semeval, False)

"""##### WE:1 Analysis on Glove dataset, Wiki corpus"""

w = w_gv_1
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

data = data_google
calAnswersOnSampleData(data, True, w)

data = data_msr
calAnswersOnSampleData(data, True, w)

"""##### WE:2"""

w = w_gcs_1
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:3"""

w = w_gv_2
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:4"""

w = w_gv_3
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:5"""

w = w_gv_4
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:6"""

w = w_gcbow_1
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:7"""

w = w_gcs_2
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:8"""

w = w_pdc
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:9"""

w = w_lexvev
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:10"""

w = w_concept_net
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:11"""

w = w_hdc
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)

"""##### WE:12"""

w = w_fasttext
results_wordrep = calAnswersOnWordRep(data_wordrep, w, max_pairs=5)
print(results_wordrep)

results = evaluateOnAll(w)
print(results)